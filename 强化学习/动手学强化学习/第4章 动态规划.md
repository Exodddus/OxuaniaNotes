基于动态规划的强化学习算法主要有两种：一是**策略迭代**（policy iteration），二是**价值迭代**（value iteration）。
**策略迭代**由两部分组成：*策略评估*（policy evaluation）和*策略提升*（policy improvement）。*策略评估*使用贝尔曼期望方程来得到一个策略的状态价值函数，这是一个动态规划的过程。
**价值迭代**直接使用贝尔曼最优方程来进行动态规划，得到最终的最优状态价值。
条件：
1. 基于动态规划的这两种强化学习算法要求事先知道环境的状态转移函数和奖励函数，也就是需要知道整个马尔可夫决策过程。在这样一个白盒环境中，不需要通过智能体和环境的大量交互来学习，可以直接用动态规划求解状态价值函数。但是，现实中的白盒环境是极少的。
2. 通常只适用于有限马尔可夫决策过程，即状态空间和动作空间是离散且有限的。

# 悬崖漫步
![[CliffWalking.jpg]]