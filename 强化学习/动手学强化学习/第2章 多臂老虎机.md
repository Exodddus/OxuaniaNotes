# 定义
有一个拥有$K$根拉杆的老虎机，拉动每一根拉杆都对应一个关于奖励的概率分布$R$。每次拉动其中一根拉杆，就可以从该拉杆对应的奖励概率分布中获得一个奖励$r$ 。我们在各根拉杆的奖励概率分布未知的情况下，从头开始尝试，目标是在操作$T$次拉杆后获得尽可能高的累积奖励。
- 需要在“探索拉杆的获奖概率”和“根据经验选择获奖最多的拉杆”中进行权衡

# 描述
假设最优期望$Q^*=max_{a\in A}Q(a)$，定义**懊悔**为当前拉杆与最优拉杆期望奖励差，$R(a)=Q^*-Q(a)$，累计懊悔$\sigma_R=\sum_{t=1}^T R(a_t)$。目标最大化累计奖励即等价于最小化累计懊悔。

初始化计数器 $N(a)=0$ 和期望奖励估值 $\hat Q=0$，每次执行一次操作后更新计数器$N(a_t)=N(a_t)+1$期望估值$\hat{Q}(a_t)=\frac{1}{N(a_t)}[r_t - \hat{Q}(a_t)]$

# $\epsilon$-贪心算法
每次以概率 $1-\epsilon$ 选择以往经验中期望奖励估值最大的那根拉杆（利用），以概率 $\epsilon$ 随机选择一根拉杆（探索）。随着探索次数的不断增加，我们对各个动作的奖励估计得越来越准，所以具体实现中可以令$\epsilon$随时间衰减。
通过实验可以发现，如果$\epsilon$是一个固定值，那么累计懊悔线性增长；如果取$\epsilon_t = \frac{1}{t}$，累积懊悔与时间步的关系将变成*次线性*（sublinear）的。

# 上置信界算法
一根拉杆的不确定性越大，它就越具有探索的价值，因为探索之后我们可能发现它的期望奖励很大。
数学原理：**霍夫丁不等式** $\mathbb{P}\{\mathbb{E}[X] \geq \bar{x}_n + u\} \leq e^{-2 n u^2}$。给定一个概率 $p=e^{-2N_t(a) U_t(a)^2}$，反解可以得到$\hat{U}_t(a)$。我们认为$\hat{Q}_t(a) + \hat{U}_t(a)$即为拉杆$a$的上界。每次选择奖励上界最大的拉杆。

# 汤普森采样
假设每根拉杆的奖励服从一个特定的概率分布，然后根据拉动每根拉杆的期望奖励来进行选择。
汤普森采样算法使用采样的方式，即根据当前每个动作 $a$ 的奖励概率分布进行一轮采样，得到一组各根拉杆的奖励样本，再选择样本中奖励最大的动作。