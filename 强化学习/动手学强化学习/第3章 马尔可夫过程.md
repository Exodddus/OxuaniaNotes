# 随机过程
某时刻 $t$ 的状态取决于 $t$ 之前的状态。将已知历史信息$(S_1,...,S_t)$ 时下一个时刻状态为 $S_{t+1}$ 的概率表示成 $P(S_{t+1} \mid S_1, \dots, S_t)$。

# 马尔可夫性质
$P(S_{t+1} \mid S_t) = P(S_{t+1} | S_1, \dots, S_t)$

# 马尔可夫过程

状态转移矩阵 $\mathcal{P}$ : 

$\mathcal{P} = \begin{bmatrix} P(s_1|s_1) & \cdots & P(s_n|s_1) \\ \vdots & \ddots & \vdots \\ P(s_1|s_n) & \cdots & P(s_n|s_n) \end{bmatrix}$
$P(s_j|s_i)$ 表示从状态 $s_i$ 转移到状态 $s_j$ 的概率。
给定一个马尔可夫过程，我们就可以从某个状态出发，根据它的状态转移矩阵生成一个状态**序列**（episode），这个步骤也被叫做**采样**（sampling）。生成这些序列的概率和状态转移矩阵有关。

# 马尔可夫奖励过程
一个马尔可夫奖励过程由$<\mathcal{S}, \mathcal{P}, r, \gamma>$构成。$\mathcal{S}$是有限状态的集合，$r$ 是奖励因子，$r(s)$ 是转移到该状态时可以获得奖励的期望。$\gamma$ 是折扣因子，有时需要对远期利益大一些折扣，接近1的 $\gamma$ 关注长期累计奖励，接近0的话关注短期奖励。

## 回报
**回报** $G_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k}$

## 价值函数
**贝尔曼方程** $V(s) = r(s) + \gamma \sum_{s' \in S} p(s' | s) V(s')$
矩阵形式 $\mathcal{V} = \mathcal{R} + \gamma \mathcal{P} \mathcal{V}$
解析解 $\mathcal{V} = (I - \gamma \mathcal{P})^{-1} \mathcal{R}$
- 计算复杂度 $O(n^3)$，有些太大了

# 马尔可夫决策过程
不同于马尔可夫奖励过程，在马尔可夫决策过程中，通常存在一个智能体来执行动作。

**状态价值函数** $V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right]$
**动作价值函数** $Q^\pi(s,a) = \mathbb{E}_{\pi} [G_t | S_t = s, A_t = a] = r(s, a) + \gamma \sum_{s'\in S}P(s' | s, a) V^{\pi}(s')$
## 贝尔曼期望方程

$\begin{align*} V^\pi(s) &= \mathbb{E}_\pi \left[ R_t + \gamma V^\pi(S_{t+1}) \mid S_t = s \right] \\ &= \sum_{a \in A} \pi(a|s) \left( r(s,a) + \gamma \sum_{s' \in S} p(s'|s,a) V^\pi(s') \right) \\ Q^\pi(s,a) &= \mathbb{E}_\pi \left[ R_t + \gamma Q^\pi(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a \right] \\ &= r(s,a) + \gamma \sum_{s' \in S} p(s'|s,a) \sum_{a' \in A} \pi(a'|s') Q^\pi(s', a') \end{align*}$

- 给定一个 MDP 和一个策略 $\pi$，我们是否可以将其转化为一个 MRP？答案是肯定的。
对于某一个状态，我们根据策略所有动作的概率进行加权，得到的奖励和就可以认为是一个 MRP 在该状态下的奖励。
$r'(s) = \sum_{a \in A} \pi (a|s) r(s, a)$

# 蒙特卡洛方法
也被称为统计模拟方法，是一种基于概率统计的数值计算方法。运用蒙特卡洛方法时，我们通常使用重复随机抽样，然后运用概率统计方法来从抽样结果中归纳出我们想求的目标的数值估计。
- 直观例子：$$ \frac{圆的面积}{正方形的面积} = \frac{圆中点的个数}{正方形中点的个数} $$
估计一个策略在某个决策过程中的状态价值函数，可以在 MDP 上采样多条序列，计算从这个状态出发的回报再求其期望。
$$ V^\pi(s) = \mathbb{E}_\pi \left[ G_t \mid S_t = s \right] \approx \frac{1}{N} \sum_{i=1}^N G_t^{(i)} $$

# 占用度量
不同策略会使智能体访问到不同概率分布的状态。
首先定义MDP初始状态分布为$v_0(s)$，用$P^{\pi}_t(s)$表示采取策略$\pi$是的智能体在 $t$ 时刻状态为 $s$ 的概率。
**状态访问分布**: $\nu^\pi(s) = (1 - \gamma) \sum_{t=0}^{\infty} \gamma^t P_t^\pi(s)$
动作状态对$(s,a)$被访问到的概率**占用度量** $$\rho
^{\pi}(s,a) = \nu^{\pi}(s) \pi (a \mid s)$$ $\rho^{\pi}(s,a) = \nu^{\pi}(s)\pi(a|s)$

# 最优策略
最优状态价值函数和最优动作价值函数之间的关系 $Q^*(s,a) = r(s,a) + \gamma \sum_{s' \in S} P(s'|s,a) V^*(s')$

**贝尔曼最优方程**：$\begin{align*} V^*(s) &= \max_{a \in \mathcal{A}} \left\{ r(s,a) + \gamma \sum_{s' \in S} p(s'|s,a) V^*(s') \right\} \\ Q^*(s,a) &= r(s,a) + \gamma \sum_{s' \in S} p(s'|s,a) \max_{a' \in \mathcal{A}} Q^*(s', a') \end{align*}$
